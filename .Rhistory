df_class = cbind(df_ptsCaract, class)%>%filter(class == class_test)
resultats = rep(0, dim(df_class)[1])
# tester csurface (plus simple)
for (i in 1:dim(df_class)[1]){
if(df_class$csurf[i] - 2 <= prMoy["csurf"]%>%unname() && df_class$csurf[i] + 2 >= prMoy["csurf"]%>%unname()){
#resultats [i] = 1
# tester nbExtrema
if(df_class$nbExtrema[i] - 1 <= prMoy["nbExtrema"]%>%unname() && df_class$nbExtrema[i] + 1 >= prMoy["nbExtrema"]%>%unname()){
#resultats [i] = 1
# tester finalement cfond
if(df_class$profondeur[i] <= 1000){
if(df_class$cfond[i] - 2 <= prMoy["cfond"]%>%unname() && df_class$cfond[i] + 2 >= prMoy["cfond"]%>%unname()){
resultats [i] = 1
}
}
else {
if((df_class$cfond[i] - 0.5 <= prMoy["cfond"]%>%unname() && df_class$cfond[i] + 0.5 >= prMoy["cfond"]%>%unname())){
resultats[i] = 1
}
}
}
}
}
}
else{
df_class = cbind(df_ptsCaract, class)%>%filter(class == class_test)
resultats = rep(0, dim(df_class)[1])
# tester csurface (plus simple)
for (i in 1:dim(df_class)[1]){
if(df_class$csurf[i] - 2 <= prMoy["csurf"]%>%unname() && df_class$csurf[i] + 2 >= prMoy["csurf"]%>%unname()){
#resultats [i] = 1
# tester nbExtrema
if(df_class$nbExtrema[i] - 1 <= prMoy["nbExtrema"]%>%unname() && df_class$nbExtrema[i] + 1 >= prMoy["nbExtrema"]%>%unname()){
#resultats [i] = 1
# tester finalement cfond
if(df_class$profondeur[i] <= 1000){
if(df_class$cfond[i] - 2 <= prMoy["cfond"]%>%unname() && df_class$cfond[i] + 2 >= prMoy["cfond"]%>%unname()){
resultats [i] = 1
}
}
else {
if((df_class$cfond[i] - 0.5 <= prMoy["cfond"]%>%unname() && df_class$cfond[i] + 0.5 >= prMoy["cfond"]%>%unname())){
resultats[i] = 1
}
}
}
}
}
}
return(resultats)
}
f1Score = function(df_ptsCaract, class, class_test_){
# return le score f1 d'une classe
score_class = test_similarite_classe(df_ptsCaract, class, class_test_)
vp = sum(score_class)
fp = length(score_class) - vp
fn = 0
df = cbind(df_ptsCaract, class)
for (i in df$class[!df$class == class_test_]%>%unique()){
score_class = test_similarite_classe(df_ptsCaract, class, class_test = i, prMoy = profil_moyen_function(df_ptsCaract, class)[[class_test_]])
fn = fn + sum(score_class)
}
f1 = 2*vp/(2*vp + fp + fn)
return(f1)
}
gcv_MPC_bloc = function(bloc, lambda, base){
if(is.vector(bloc)){
mat = smooth.basis(as.numeric(names(bloc)), bloc%>%unname(), fdPar(base, 2, lambda = lambda))$gcv
} else{
mat = smooth.basis(as.numeric(colnames(bloc)), bloc%>%t(), fdPar(base, 2, lambda = lambda))$gcv
}
return(mat)
}
spline_lissage_bloc = function(bloc, l_grille, D){
# bloc pas de valeurs manquantes
# gestion du cas un bloc d'un vecteur
if (is.vector(bloc)){
base = create.bspline.basis(rangeval = range(as.numeric(names(bloc))), nbasis = D)
mat = matrix(0, nrow = 1, ncol = length(l_grille))
for (i in 1:length(l_grille)){
mat[, i] = gcv_MPC_bloc(bloc, l_grille[i], base)
}
# lambda optimal pour chaque courbe
lambda_optimaux = l_grille[which.min(mat%>%c())]
liss = smooth.basis(as.numeric(names(bloc)), bloc%>%unname() , fdPar(base, 2, lambda = lambda_optimaux))$fd
bloc_hat = eval.fd(as.numeric(names(bloc)), liss)
coefs_bloc = matrix(liss$coefs, ncol = D)
rmse = sqrt(mean((bloc_hat - bloc)^2))
}else{
base = create.bspline.basis(rangeval = range(as.numeric(colnames(bloc))), nbasis = D)
# bloc --> matrice
mat = matrix(0, nrow = dim(bloc)[1], ncol = length(l_grille))
for (i in 1:length(l_grille)){
mat[, i] = gcv_MPC_bloc(bloc, l_grille[i], base)
}
# lambda optimal pour chaque courbe
lambda_optimaux = l_grille[apply(mat, MARGIN = 1, FUN = which.min)]
bloc_hat = matrix(0, ncol = dim(bloc)[2], nrow = dim(bloc)[1])
coefs_bloc = matrix(0, nrow = dim(bloc)[1], ncol =  D)
rmse = rep(0, dim(bloc)[1])
for (i in 1:dim(bloc)[1]){
y = bloc[i, ]
liss = smooth.basis(as.numeric(colnames(bloc)), y , fdPar(base, 2, lambda = lambda_optimaux[i]))$fd
bloc_hat[i, ] = eval.fd(as.numeric(colnames(bloc)), liss)
coefs_bloc[i, ] = liss$coefs
rmse[i] = sqrt(mean((bloc_hat[i, ] - bloc[i, ])^2))
}
}
return(list(bloc = bloc_hat, coefs = coefs_bloc, rmse = rmse))
}
spline_lissage_complet = function(data, taille_courbes, l_grille, D){
# fournit un lissage des données par b_spline
coefs = matrix(NA, nrow = dim(data)[1], ncol = D)
yhat = matrix(NA, nrow = dim(data)[1], ncol = dim(data)[2])
rmse = c()
unique_taille = taille_courbes%>%unique()%>%sort() # differents longueur de profil
# premier stockage
bloc = data[which(taille_courbes == unique_taille[1]), 1:unique_taille[1]]
suivi_idx = which(taille_courbes == unique_taille[1]) # !!!! important
sp = spline_lissage_bloc(bloc, l_grille, D)
if(is.vector(bloc)){
yhat[1, 1:length(bloc)] = sp$bloc
coefs[1, ] = sp$coefs
rmse = sp$rmse
index = 1
}else{
yhat[1:dim(bloc)[1], 1: dim(bloc)[2]] = sp$bloc
coefs[1:dim(bloc)[1], ] = sp$coefs
rmse = sp$rmse
index = dim(bloc)[1]
}
# boucler
for (i in 2:length(unique_taille)){
bloc = data[which(taille_courbes == unique_taille[i]), 1:unique_taille[i]]
suivi_idx = c(suivi_idx, which(taille_courbes == unique_taille[i]))
# cas d'un vecteur
sp = spline_lissage_bloc(bloc, l_grille, D)
if(is.vector(bloc)){
yhat[(index + 1):(index + 1), 1:length(bloc)] = sp$bloc
coefs[(index + 1):(index + 1), ] = sp$coefs
rmse = c(rmse, sp$rmse)
index = index + 1
} else{
yhat[(index + 1):(index + dim(bloc)[1]), 1:dim(bloc)[2]] = sp$bloc
coefs[(index + 1):(index + dim(bloc)[1]), ] = sp$coefs
rmse = c(rmse, sp$rmse)
index = index + dim(bloc)[1]
}
}
yhat = data.frame(yhat)
colnames(yhat) = colnames(data)
coefs = data.frame(coefs)
yhat = cbind(yhat, idx = suivi_idx)
coefs = cbind(coefs, idx = suivi_idx)
# remise à jour
yhat = yhat%>%arrange(idx)%>%dplyr::select(!idx)
coefs = coefs%>%arrange(idx)%>%dplyr::select(!idx)
# reset index
rownames(yhat) = 1:dim(yhat)[1]
rownames(coefs) = 1:dim(coefs)[1]
return(list(data = yhat, coefs = coefs, rmse = rmse))
}
gg_profil = function(yhat){
# prends en paramètre un data avec les profondeurs comme nom de colonne du tableau
yhat$ind = paste0("i", 1:dim(yhat)[1])
gg = yhat%>%pivot_longer(cols = yhat%>%dplyr::select(!ind)%>%colnames())%>%ggplot(aes(x = -as.numeric(name), y = value,  color = ind))+
geom_line()+
coord_flip()+
theme_minimal()+
labs(title = "", x = "profondeur (m/s)", y = "célérité (m/s)")+
theme(legend.position = "none")
return(gg)
}
ptsCaract_lissage = function(lissage, data, taille_courbes){
# même base que le lissage
# prends en params les données lissées
# prends en compte tailles_courbes pour les célérités au fond (?données lissées ou non)
# retourne data frame des points caractéristiques et les coefficients de lissage
#base = create.bspline.basis(rangeval = c(0, 2500), nbasis = 30)
csurf = data[, 1]
cfond = sapply(1:dim(data)[1], function(i) data[i, taille_courbes[i]])
profondeur = as.numeric(colnames(data))[taille_courbes]
# reste les extremas
coefs = lissage$coefs
yhat = lissage$data
#x_prime = eval.basis(as.numeric(colnames(yhat)), base, Lfdobj = 1)%*%t(coefs)
nbExtrema = rep(0, dim(yhat)[1])
for (i in 1:dim(yhat)[1]){
lev = as.numeric(colnames(yhat))[!is.na(yhat[i, ])]
base = create.bspline.basis(rangeval = range(lev), nbasis = 30)
x_prime = eval.basis(lev, base, Lfdobj = 1)%*%t(coefs[i, ])
nbExtrema[i] = changement_signe_xprime(x_prime, lev)$nbExtrema
}
return(data.frame(csurf, cfond, nbExtrema, profondeur))
}
# regarder fpca.sc -----> données irrégulières
# idée : passer par les coefficients mais pour cela on doit disposer d'une bspline orthonormée
#      : trouver une idée de transformation des coefficients pour une bspline non orthonorméé vers des coefficients donnée par une base ortho
#      : oubien creer une base bspline orthonormer de class basis ?
# lissage optimiser
lissage = spline_lissage_complet(data, taille_courbes, l_grille = 10^seq(-3, 5, length.out = 1000) , D = 30)
saveRDS(lissage, "lissage.rds")
bloc_data = function(data, taille_courbes){
# Constituer nos données en deux blocs de variables
# - un bloc avec que des données complètes  matrix nbprofil * min(taille_courbes)
# - un dernier bloc présentant des valeurs manquantes
unique_taille = taille_courbes%>%unique()%>%sort()
idx_suivi = c()
data_bloc = matrix(0, nrow = dim(data)[1], ncol = dim(data)[2])
for (i in unique_taille){
idx = which(taille_courbes == i)
data_bloc[idx, ] = data[idx, ]
idx_suivi = c(idx_suivi, idx)
}
data_bloc = data.frame(data_bloc)
data_bloc$idx = idx_suivi
data_bloc = data_bloc%>%arrange(idx)%>%dplyr::select(!idx)
return(data_bloc)
#return(list(df = data_bloc, idx = idx_suivi))
}
# ---------------------------------------------- Méthode de discrétisation --------------------------------------------------
vect_discretisation = function(vec){
# prends une variable ---> regarde le nombre de variable manquantes (si ça dépasse pas 10%) ---> combler le manquant avec les profils de célérité les plus faible
# si missings dépasse 10% alors ont constitue une classe 0 pour les missings et pour le reste on teste les decoupage possible --> via les quantiles
vec = data.frame(vec, idx = 1:length(vec))
vec = vec%>%arrange(desc(vec))
seuil = as.integer(vec[, 1]%>%length() *0.1) + 1
idx0 = which(vec$vec%>%is.na()) # initialiser les indices des valeurs manquantes
if(length(idx0) < seuil){
vec$vec[rownames(tail(vec, seuil)["vec"])%>%as.integer()] = 0
# regarder ensuite le summary du vecteur
summary_vec = (summary(vec$vec[which(vec$vec != 0)])%>%unname())[-4] # on enlève la moyenne
# regarder si il y'a 10% d'observations entre les quartiles
vec$vec[which(vec$vec >= summary_vec[1] & vec$vec <= summary_vec[2])] = 1
vec$vec[which(vec$vec > summary_vec[2] & vec$vec <= summary_vec[3])] = 2
vec$vec[which(vec$vec > summary_vec[3])] = 3
} else{
vec$vec[idx0] = 0
res = which(vec$vec != 0)%>%length()
if((res/3) > seuil){
summary_vec = (summary(vec$vec[which(vec$vec != 0)])%>%unname())[-4]
vec$vec[which(vec$vec >= summary_vec[1] & vec$vec <= summary_vec[2])] = 1
vec$vec[which(vec$vec > summary_vec[2] & vec$vec <= summary_vec[3])] = 2
vec$vec[which(vec$vec > summary_vec[3])] = 3
}
else if ((res/2) > seuil){
summary_vec = (summary(vec$vec[which(vec$vec != 0)])%>%unname())[-4]
vec$vec[which(vec$vec >= summary_vec[1] & vec$vec <= summary_vec[3])] = 1 # plage min <-> median 50% des obs (res/2 > seuil)
vec$vec[which(vec$vec > summary_vec[3])] = 2
}
else if ((res > seuil)){
vec$vec[which(vec$vec != 0)] = 1
}
else{
vec$vec[which(vec$vec != 0)] = 0 # tous les données sont de la classe 0
}
}
return(vec)
}
# evaluation de la partition par vec
eval_discret = function(vec_discret, vec){
# evalue le partitionnemment de chaque vecteur ---> via la variance intra classe
unique_class = vec_discret%>%unique()%>%sort()
eval  = rep(0, length(unique))
for (i in 1:length(unique_class)){
eval[i] = vec[which(vec_discret == (i-1))]%>%var(na.rm = TRUE) # si ya des NA
}
return(eval)
}
gg_eval_discret = function(data, bloc_discret){
data_quali = data[, -c(1:7)]
mat = matrix(0, nrow = 4, ncol = dim(data_quali)[2])
for (j in 1:dim(data_quali)[2]){
vec = data_quali[, j]
vec_discret = vect_discretisation(data[, j])%>%arrange(idx)%>%select(!idx)%>%unlist()%>%unname()
mat[, j] = eval_discret(vec_discret, vec)
}
mat = cbind(class = c(0, 1, 2, 3), data.frame(mat))
gg = pivot_longer(mat, cols = colnames(mat)[-1])%>%ggplot(aes(x = factor(name), y = value, fill = factor(class)))+
geom_bar(stat = "identity", position = "dodge2")+
theme_minimal()+
labs(x = "", y = "Variance intra-classe (m/s)", fill = "Classes")
return(gg)
}
# tester
#vec_discret = vect_discretisation(data[, 40])%>%arrange(idx)%>%select(!idx)%>%unlist()%>%unname()
#vec = data[, 40]
#eval_discret(vec_discret, vec)
# graphique qui montre les résultats de la discrétisation ----> pour faire des diagramme en barr pour chaque variable
dicretisation = function(data_bloc){
idx_fin = which(apply(data_bloc, 2, FUN = function(x) x%>%is.na()%>%sum() > 0) == TRUE)[1] # indice variable à la quelle on commence à avoir des données manquantes
bloc_discret = data_bloc[, idx_fin : dim(data_bloc)[2]]
mat = matrix(NA, ncol = dim(bloc_discret)[2], nrow = dim(bloc_discret)[1])
for (j in 1:dim(bloc_discret)[2]){
vec = vect_discretisation(bloc_discret[, j]) # possede idx pour ré-arranger
mat[, j] = vec%>%arrange(idx)%>%dplyr::select(!idx)%>%unname()%>%unlist()
}
return(mat)
}
bloc_discret = dicretisation(data_bloc)
gg_discretisation = function(bloc_discret){
# prends les données discrediter quali
mat = matrix(NA, ncol = 4, nrow = dim(bloc_discret)[2])
for (j in 1:dim(bloc_discret)[2]){
mat[j, ] = bloc_discret[, j]%>%table()%>%unname()
}
mat = data.frame(mat)
colnames(mat) = c("0", "1", "2", "3")
vizu = cbind(var = paste0("var", 1:dim(bloc_discret)[2]), mat)
gg = pivot_longer(vizu, cols = colnames(mat))%>%ggplot(aes(x = factor(var, levels = paste0("var", 1:dim(bloc_discret)[2])), y = value, fill = name))+
geom_bar(stat = "identity", position = "stack")+
geom_text(aes(label = paste0(round(value*100/dim(bloc_discret)[1], 0), "%")), position = position_stack(vjust = 0.5))+
theme_minimal()+
labs(x = "", y = "", fill = "Classes")
return(gg)
}
gg_discretisation(bloc_discret)
resultat_discretisation = function(data, taille_courbes){
# agregation des données quali et quanti
sorti_bloc = bloc_data(data, taille_courbes) # ---> variable quanti + variable quali
discret_bloc  = dicretisation(sorti_bloc) # -----> matrice des données qualitatives (ignore les quantis)
# regrouper les deux boc ----> quanti + quali
finaldata = cbind(sorti_bloc[, 1:(dim(data)[2] - dim(discret_bloc)[2])], discret_bloc)
colnames(finaldata) = colnames(data)
return(finaldata)
}
finaldata = resultat_discretisation(data, taille_courbes)
# regarder l'algo de fisher ----> référence de madame Niang
# Appliquer l'AFDM là dessus
# comparer la discrétisation au vrai valers ---> 1000 observation
pheatmap(data[0:2000, 8:47]%>%is.na()*1, cluster_rows = FALSE, cluster_cols = FALSE, color = colorRampPalette(c("blue", "white", "red"))(50))
pheatmap(bloc_discret[0:2000, ], cluster_rows = FALSE, cluster_cols = FALSE, color = colorRampPalette(c("red", "white", "blue"))(50))
#-------------------------------- pts caractéristiques version 2 -----------------------------------------------------------------
# gcv = function(lambda){smooth.basis(lev, vec%>%unname(), fdPar(create.bspline.basis(range(lev), nbasis = 30), 2, lambda = lambda))$gcv}
#
# ptsCaract_lissage_by_curve = function(vec, l_grille){
#   vec = vec[which(!vec%>%is.na())]
#   lev = as.numeric(names(vec))
#   lambda_opti = l_grille[which.min(sapply(l_grille, gcv))]
#   coefs = smooth.basis(as.numeric(names(vec)), vec%>%unname(), fdPar(create.bspline.basis(range(lev), nbasis = 30), 2, lambda = lambda_opti))$fd$coefs
#   phi_prime = eval.basis(as.numeric(names(vec)), create.bspline.basis(range(lev), nbasis = 30), Lfdobj = 1)
#   x_prime = phi_prime%*%coefs
#   return(changement_signe_xprime(x_prime, lev)$nbExtrema)
# }
#
# #ptsCaract_lissage_by_curve(data_[1, ], l_grille)
#
# library(doParallel)
# library(foreach)
#
# ptsCaract_lissage_complet = function(data, l_grille){
#   csurf = data[, 1]
#   cfond = sapply(1:dim(data)[1], function(i) data[i, taille_courbes[i]])
#   profondeur = as.numeric(colnames(data))[taille_courbes]
#   #nbExtrema = rep(0, dim(data)[1])
#   cl = makePSOCKcluster(detectCores() - 2)
#   clusterExport(cl, c("data", "l_grille", "ptsCaract_lissage_by_curve", "gcv"))
#   nbExtrema = foreach(i = 1:dim(data)[1], .combine = 'c', .packages = c("tidyverse", "fda"))%dopar%{
#     ptsCaract_lissage_by_curve(data[i, ], l_grille)
#   }
#   stopCluster(cl)
#
#   #df = data.frame(csurf, cfond, nbExtrema, profondeur)
#   return(nbExtrema)
# }
#
# ptsCaract_lissage_adapt = ptsCaract_lissage_complet(data = data[1:100, ], l_grille)
#
#
#
paths  = paste0("./", list.files("data"))
nom =  paste0("var_SV_2018_01_01_", c(00, 06, 12, 18), "H")
lire_data = function(path){
data = read.csv2(path, na.strings = "")%>%dplyr::select(!pixel)%>%as.data.frame()
colnames(data) = gsub("^X", "", colnames(data))
data = apply(data, 2, FUN = as.numeric)
return(data)
}
data <- lire_data(path)
View(data)
if (length(paths) > 0) {
data <- lire_data(paths[1])
View(data)
} else {
print("Aucun fichier trouvé dans le répertoire spécifié.")
}
if (length(paths) > 0) {
data <- lire_data(paths[1])
View(data)
} else {
print("Aucun fichier trouvé dans le répertoire spécifié.")
}
if (length(paths) > 0) {
data <- lire_data(paths[1])
print("Dossier trouvé...")
View(data)
} else {
print("Aucun fichier trouvé dans le répertoire spécifié.")
}
# exemple 1:
paths  = paste0("./", list.files("data"))
nom =  paste0("var_SV_2018_01_01_", c(00, 06, 12, 18), "H")
lire_data = function(path){
data = read.csv2(path, na.strings = "")%>%dplyr::select(!pixel)%>%as.data.frame()
colnames(data) = gsub("^X", "", colnames(data))
data = apply(data, 2, FUN = as.numeric)
return(data)
}
if (length(paths) > 0) {
data <- lire_data(paths[1])
print("Data trouvé !!!")
View(data)
} else {
print("Aucun fichier trouvé dans le répertoire spécifié.")
}
library(dplyr)
library(tidyverse)
# Chemins d'accès aux fichiers
paths <- paste0("./", list.files("data"))
# Noms des fichiers
noms <- paste0("var_SV_2018_01_01_", c(00, 06, 12, 18), "H")
# Fonction pour lire les données
lire_data <- function(path) {
data <- read.csv2(path, na.strings = "") %>%
dplyr::select(!pixel) %>%
as.data.frame()
colnames(data) <- gsub("^X", "", colnames(data))
data <- apply(data, 2, FUN = as.numeric)
return(data)
}
# Lire le premier fichier pour tester
if (length(paths) > 0) {
data <- lire_data(paths[1])
View(data)
} else {
print("Aucun fichier trouvé dans le répertoire spécifié.")
}
data <- read.csv2("C:/Users/hugo.nguyen/Documents/M2 TRIED/Projet_Long/data/var_SV_2018-01-01_00H_nan-forced_depth.csv", na.strings = "")%>%dplyr::select(!pixel)%>%as.data.frame()
colnames(data) = gsub("^X", "", colnames(data))
data = apply(data, 2, FUN = as.numeric)
data()
View(Data)
data <- read.csv2("C:/Users/hugo.nguyen/Documents/M2 TRIED/Projet_Long/data/var_SV_2018-01-01_00H_nan-forced_depth.csv", na.strings = "")%>%dplyr::select(!pixel)%>%as.data.frame()
data <- read.csv2("C:/Users/hugo.nguyen/Documents/M2 TRIED/Projet_Long/data/var_SV_2018-01-01_00H_nan-forced_depth.csv", na.strings = "")
data <- read.csv2("C:/Users/hugo.nguyen/Documents/M2 TRIED/Projet_Long/data/var_SV_2018-01-01_00H_nan-forced_depth.csv", na.strings = "")
colnames(data) = gsub("^X", "", colnames(data))
data = apply(data, 2, FUN = as.numeric)
file_path <- "C:/Users/hugo.nguyen/Documents/M2 TRIED/Projet_Long/data/var_SV_2018-01-01_00H_nan-forced_depth.csv"
data <- read.csv(file_path, na.strings = "")
# Vérifier si le fichier a été lu correctement
if (nrow(data) == 0) {
stop("Le fichier est vide ou n'a pas été lu correctement.")
}
# Renommer les colonnes
colnames(data) <- gsub("^X", "", colnames(data))
# Convertir les colonnes en numérique
data <- apply(data, 2, FUN = function(column) as.numeric(as.character(column)))
# Vérifier si la conversion a réussi
if (any(sapply(data, is.na))) {
warning("Certaines valeurs n'ont pas pu être converties en numérique.")
}
# Afficher les premières lignes du dataframe pour vérifier
head(data)
data.table
# Lire le fichier CSV
file_path <- "C:/Users/hugo.nguyen/Documents/M2 TRIED/Projet_Long/data/var_SV_2018-01-01_00H_nan-forced_depth.csv"
data <- read.csv(file_path, na.strings = "")
# Vérifier si le fichier a été lu correctement
if (nrow(data) == 0) {
stop("Le fichier est vide ou n'a pas été lu correctement.")
}
# Renommer les colonnes
colnames(data) <- gsub("^X", "", colnames(data))
# Convertir les colonnes en numérique
data <- apply(data, 2, FUN = function(column) as.numeric(as.character(column)))
# Vérifier si la conversion a réussi
if (any(sapply(data, is.na))) {
warning("Certaines valeurs n'ont pas pu être converties en numérique.")
}
# Afficher les premières lignes du dataframe pour vérifier
head(data)
# Lire le fichier CSV
file_path <- "C:/Users/hugo.nguyen/Documents/M2 TRIED/Projet_Long/data/var_SV_2018-01-01_00H_nan-forced_depth.csv"
data <- read.csv(file_path, na.strings = "")
# Vérifier si le fichier a été lu correctement
if (nrow(data) == 0) {
stop("Le fichier est vide ou n'a pas été lu correctement.")
}
# Renommer les colonnes
colnames(data) <- gsub("^X", "", colnames(data))
# Convertir les colonnes en numérique
data <- apply(data, 2, FUN = function(column) as.numeric(as.character(column)))
# Vérifier si la conversion a réussi
if (any(sapply(data, is.na))) {
warning("Certaines valeurs n'ont pas pu être converties en numérique.")
}
# Afficher les premières lignes du dataframe pour vérifier
head(data)
library(readr)
data <- read_delim("C:/Users/hugo.nguyen/Documents/M2 TRIED/Projet_Long/data/var_SV_2018-01-01_00H_nan-forced_depth.csv",
delim = ";", escape_double = FALSE, trim_ws = TRUE)
View(data)
library(readr)
data <- read_delim("C:/Users/hugo.nguyen/Documents/M2 TRIED/Projet_Long/data/var_SV_2018-01-01_00H_nan-forced_depth.csv",
delim = ";", escape_double = FALSE, trim_ws = TRUE)
View(data)
library(readr)
data <- read_delim("C:/Users/hugo.nguyen/Documents/M2 TRIED/Projet_Long/data/var_SV_2018-01-01_00H_nan-forced_depth.csv",
delim = ";", escape_double = FALSE, trim_ws = TRUE)
View(data)
setwd("C:/Users/hugo.nguyen/Documents/M2 TRIED/Projet_Long")
data <- read.csv(file_path, na.strings = "")
